less than a mile from the house of commons is the metropolitan police's secretive forensic unit. set inside a monolithic, 1980s concrete shell, the lab handles everything from deconstructing firearms to the tricky process of rebuilding destroyed mobile phones. but it's increasingly grappling with a recent problem: our explosion of data.

modern crime can't be separated from the messy trail of data it leaves behind. even when cases aren't explicitly cybercrime, they're linked to messages sent through social networks. mark stokes, the force's head of digital forensics, estimates that his teams are obtaining more than half a petabyte of data every six months – equivalent to 250 billion pages of typed text.


where the problem of excessive data is exacerbated is in cases involving images and videos of child sexual abuse. "a single case can have terabytes of data storage," stokes says. this can be split across smartphones, multiple laptops and external hard-drives. "we've seen massive gaming machines, and [suspects] tend to just have more high powered machines with more storage."

now, police in the uk are turning to deep learning to help with the deluge of child abuse images discovered on devices. the country's child abuse image database (caid), which was created in 2014, contains more than 13 million images and the number is rapidly increasing: more than half a million new images are added every six months.

the online demand for images and video of child sexual abuse is unwavering. over the course of 2018, 105,047 urls hosting child abuse content were removed from the internet. each of the websites can contain thousands of pictures and footage. in total, since 1996, the internet watch foundation (iwf), a uk-based charity, has stripped 477,595 webpages containing abuse images from the web. a sharp increase in removals in the last few years has been attributed to better detection and faster purging of abuse images (the record for a takedown is four minutes after it was reported).

all of the uk's 40-plus police forces are being given access to tools that will speed up the process of finding abuse images and result in individual police officers spending less time trawling through graphic images. at present officers look at thousands of images to categorise them based on the levels of abuse they contain. the task is psychologically taxing, with specialist investigators spending hours sifting through abuse images.

step in, deep learning. image recognition isn't a new task for the branch of artificial intelligence, however child sexual abuse provides a more complex technological problem than standard image recognition tasks. "it's a much more difficult classification problem than a normal image," ben gancz, the ceo of london-based ai firm qumodo and co-founder of vigil ai. qumodo has developed a new digital media examiner system that exploits several ai elements. one of these is the vigil ai software which detects and categorises child abuse images. both companies developed the technologies for the police forces alongside the home office. "it's not a classical classification of: this is a cat or a dog."

instead, images depicting child abuse are harder for a machine to detect. "the vigil ai part of the system can recognise whether an image contains child abuse and then it can also see how severe that child abuse is based on the uk standards," gancz explains. images of abuse are ranked in three categories based-on their severity: a, involving penetrative sexual activity; b, which includes non-penetrative sexual activity and c, which are other indecent images.

gancz, a former police officer at the national crime agency, refuses to explain exactly how his system detects types of abuse, to avoid tipping criminals off about possible ways to sidestep the technology. however, he says several types of machine learning are used within the system that's been developed.

the system also uses artificial intelligence to match faces and locations across the already vast caid database containing millions of images. this way police officers can identify whether a young person or the geographic area the image was taken, pulled from image file gps metadata, has already been linked to an instance of abuse. (recent related work from the university of dundee has been using the hand markings and genetic patterns of abusers to identify them in pictures and videos). gancz likens his system to a search engine for the data connected to child abuse images.

the data the artificial intelligence was trained upon to identify abuse comes from previous police investigations, where the images were labelled with the type of offence committed. the ai developers only accessed the police data within secure sites.

one key component is age determination. "when someone's approaching the age of 18, it's very difficult for a human to recognise whether someone's an adult or a child," gancz says. through the previously labelled data the system is able to determine whether an image contains an image of an adult or a child.

at a demonstration at the met's forensics labs, containing non-abuse images, the detection system is set to have a confidence ratio of 95 per cent. once the system has determined whether an image may contain child abuse it puts all potential matches into groups based-on their severity.

officers are then presented with thumbnails of the pictures which can be checked in batches, rather than individually. they are required to filter out images that aren't in the right category. "false positives are okay," says stokes, referring to the ai's ability to make mistakes. he explains that humans are needed within the process – it isn't fully automated on purpose – to verify the machine's decisions. "it's the false negatives that we always worry about – it's missing stuff."

the system allows investigators to get through multiple images in a short amount of time. initial tests of the technology in a pilot scheme found that individual members of staff could process 200 images of potential abuse in a minute, previously it was 18 images per minute. the result is that investigations can be conducted faster, with police estimates saying categorisation which may have previously taken 24 hours is now possible within 30 minutes. uk home secretary sajid javid says the levels of child sexual abuse are "off the charts". "there's some 22 cyber-related crimes against children every day," he says.

the automation by uk police has been welcomed by the iwf, which works closely with the government on dealing with child abuse. “automation/ai has a role to play in partnership with real people," says fred langford, the deputy ceo and cto at the iwf. "although ai is developing incredibly quickly it is not yet able to provide the context needed to tackle more complicated cases."

despite the tech's early use in the real world, stokes is looking at its potential for other types of crime and different still images. he says it could be used to detect knives or guns. during the 2012 murder investigation in the murder of teenager tia sharp, stokes says there were more than 20 police officers in forensics labs pouring through cctv footage linked to the case.

"it's not just about indecent images or children, it's just about being able to review imagery and videos in a much faster way," stokes says. "it could be cctv from a murder inquiry, even that cctv is getting higher resolution and better quality now, but in a murder, you could have 30 different video systems that you've downloaded. we've had some extreme examples."