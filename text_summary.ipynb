{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece as spm\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.autograd import Variable\n",
    "from torch import optim\n",
    "\n",
    "from classes import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "SENTENCEPIECE = False\n",
    "text_path = 'texts/hp_chap1.txt'\n",
    "batch_size = 10\n",
    "lr = 20  # learning rate\n",
    "epochs = 10\n",
    "clip = 0.25\n",
    "log_interval = 200\n",
    "save = 'pretrained_models/hp_model'  # path of model name you want to save\n",
    "dropout = 0.5\n",
    "n_summaries = 50  # number of summaries to generate\n",
    "seed = 42\n",
    "temperature = 2  # if this is higher, more variation in output space\n",
    "words = 1000  # number of words to generate in the word generation dump"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data, Tokenise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def form_sentence(tweet):\n",
    "    tweet_blob = TextBlob(tweet)\n",
    "    return ' '.join(tweet_blob.words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'{text_path}', 'r') as f:\n",
    "    data = f.read()\n",
    "\n",
    "if SENTENCEPIECE: \n",
    "    spm.SentencePieceTrainer.Train(f'--input={text_path} --model_prefix=m --vocab_size=3000 --model_type=bpe')\n",
    "\n",
    "    # makes segmenter instance and loads the model file (m.model)\n",
    "    sp = spm.SentencePieceProcessor()\n",
    "    sp.load('m.model')\n",
    "    ids = sp.encode_as_ids(data)\n",
    "    ntokens = 3000 # vocab size\n",
    "    bptt = 256  # sequence length\n",
    "\n",
    "    print(sp.encode_as_pieces(\"is upset that he can't update his Facebook by texting it... \" +\n",
    "                              \"and might cry as a result  School today also. Blah!\"))\n",
    "else: \n",
    "    corpus = Corpus(path=text_path)\n",
    "    ids = corpus.tokenize()\n",
    "    ntokens = len(corpus.dictionary)\n",
    "    bptt = 35"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Val Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = ids[:900]\n",
    "val_data = ids[900:1200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = torch.tensor(np.hstack(train_data))\n",
    "val_data = torch.tensor(np.hstack(val_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RNNModel(rnn_type='LSTM', ntoken=ntokens, ninp=256, nhid=512, nlayers=2, dropout=0.5, tie_weights=False)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "batch_size = 20\n",
    "eval_batch_size = 20\n",
    "train_data = batchify(train_data, batch_size)\n",
    "val_data = batchify(val_data, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   1 | time:  0.60s | valid loss 11.38 | valid ppl 87665.17\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   2 | time:  0.54s | valid loss  6.42 | valid ppl   613.27\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   3 | time:  0.51s | valid loss  6.74 | valid ppl   849.46\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   4 | time:  0.54s | valid loss  6.31 | valid ppl   550.39\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   5 | time:  0.55s | valid loss  6.27 | valid ppl   526.07\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   6 | time:  0.54s | valid loss  6.22 | valid ppl   505.20\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   7 | time:  0.53s | valid loss  6.27 | valid ppl   526.66\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   8 | time:  0.54s | valid loss  6.23 | valid ppl   505.35\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   9 | time:  0.52s | valid loss  6.22 | valid ppl   504.75\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def evaluate(data_source):\n",
    "    # Turn on evaluation mode which disables dropout.\n",
    "    model.eval()\n",
    "    total_loss = 0.\n",
    "    hidden = model.init_hidden(eval_batch_size)\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, data_source.size(0) - 1, bptt):\n",
    "            data, targets = get_batch(data_source, i)\n",
    "\n",
    "            output, hidden = model(data, hidden)\n",
    "            hidden = repackage_hidden(hidden)\n",
    "            output_flat = output.view(-1, ntokens)\n",
    "            total_loss += len(data) * criterion(output_flat, targets).item()\n",
    "    return total_loss / (len(data_source) - 1)\n",
    "\n",
    "def get_batch(source, i):\n",
    "    seq_len = min(bptt, len(source) - 1 - i)\n",
    "    data = source[i:i+seq_len]\n",
    "    target = source[i+1:i+1+seq_len].view(-1)\n",
    "    return data, target\n",
    "\n",
    "def train():\n",
    "    # Turn on training mode which enables dropout.\n",
    "    model.train()\n",
    "    total_loss = 0.\n",
    "    start_time = time.time()\n",
    "    hidden = model.init_hidden(batch_size)\n",
    "    for batch, i in enumerate(range(0, train_data.size(0) - 1, bptt)):\n",
    "        data, targets = get_batch(train_data, i)\n",
    "        # Starting each batch, we detach the hidden state from how it was previously produced.\n",
    "        # If we didn't, the model would try backpropagating all the way to start of the dataset.\n",
    "        model.zero_grad()\n",
    "        hidden = repackage_hidden(hidden)\n",
    "        output, hidden = model(data, hidden)\n",
    "        loss = criterion(output.view(-1, ntokens), targets)\n",
    "        loss.backward()\n",
    "\n",
    "        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        for p in model.parameters():\n",
    "            p.data.add_(-lr, p.grad.data)\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        if batch % log_interval == 0 and batch > 0:\n",
    "            cur_loss = total_loss / args.log_interval\n",
    "            elapsed = time.time() - start_time\n",
    "            print('| epoch {:3d} | {:5d}/{:5d} batches | lr {:02.2f} | ms/batch {:5.2f} | '\n",
    "                    'loss {:5.2f} | ppl {:8.2f}'.format(\n",
    "                epoch, batch, len(train_data) // args.bptt, lr,\n",
    "                elapsed * 1000 / args.log_interval, cur_loss, math.exp(cur_loss)))\n",
    "            total_loss = 0\n",
    "            start_time = time.time()\n",
    "\n",
    "# Loop over epochs.\n",
    "best_val_loss = None\n",
    "\n",
    "# At any point you can hit Ctrl + C to break out of training early.\n",
    "try:\n",
    "    for epoch in range(1, epochs):\n",
    "        epoch_start_time = time.time()\n",
    "        train()\n",
    "        val_loss = evaluate(val_data)\n",
    "        print('-' * 89)\n",
    "        print('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | '\n",
    "                'valid ppl {:8.2f}'.format(epoch, (time.time()-epoch_start_time),\n",
    "                                           val_loss, math.exp(val_loss)))\n",
    "        print('-' * 89)\n",
    "        # Save the model if the validation loss is the best we've seen so far.\n",
    "        if not best_val_loss or val_loss < best_val_loss:\n",
    "            with open(save, 'wb') as f:\n",
    "                torch.save(model, f)\n",
    "            best_val_loss = val_loss\n",
    "        else:\n",
    "            # Anneal the learning rate if no improvement has been seen in the validation dataset.\n",
    "            lr /= 4.0\n",
    "except KeyboardInterrupt:\n",
    "    print('-' * 89)\n",
    "    print('Exiting from training early')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use model \n",
    "## (1) Generate text in the style of the trained model \n",
    "## (2) Generate a summary of text you pass through the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sinead.cook/py/lib/python3.7/site-packages/ipykernel_launcher.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "eval_batch_size = 50  # reflective of how long you want your summary to be\n",
    "test_preprocessed = ids  # use all data (in this case) for summary\n",
    "test_data = torch.tensor(test_preprocessed)\n",
    "test_data = batchify(test_data, eval_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_encoder_outputs(data_source):  # for summary\n",
    "    # Turn on evaluation mode which disables dropout.\n",
    "    model.eval()\n",
    "    total_loss = 0.\n",
    "    hidden = model.init_hidden(eval_batch_size)\n",
    "    outputs = []\n",
    "    hiddens = {layer: [] for layer in range(len(hidden))}\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, data_source.size(0) - 1, bptt):\n",
    "            data, targets = get_batch(data_source, i)\n",
    "            emb = model.encoder(data)\n",
    "            output, hidden = model.rnn(emb, hidden)\n",
    "            outputs.append(output)\n",
    "            for layer in range(len(hidden)):\n",
    "                hiddens[layer].append(hidden[layer])\n",
    "            hidden = repackage_hidden(hidden)\n",
    "    return outputs, hiddens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs, hiddens = get_encoder_outputs(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Generated 200/1000 words\n",
      "| Generated 400/1000 words\n",
      "| Generated 600/1000 words\n",
      "| Generated 800/1000 words\n",
      "| Generated 1000/1000 words\n"
     ]
    }
   ],
   "source": [
    "# Set the random seed manually for reproducibility.\n",
    "torch.manual_seed(seed)\n",
    "device = torch.device(\"cpu\")\n",
    "model.eval()  # turn off dropout\n",
    "\n",
    "hidden = model.init_hidden(1)\n",
    "input = torch.randint(ntokens, (1, 1), dtype=torch.long).to(device)\n",
    "\n",
    "outf = 'hp_generated_dictionary.txt'\n",
    "with open(outf, 'w') as outf:\n",
    "    with torch.no_grad():  # no tracking history\n",
    "        for i in range(words):\n",
    "            output, hidden = model(input, hidden)\n",
    "            word_weights = output.squeeze().div(temperature).exp().cpu()\n",
    "            word_idx = torch.multinomial(word_weights, 1)[0]\n",
    "            input.fill_(word_idx)\n",
    "            if SENTENCEPIECE:\n",
    "                word = sp.decode_ids([word_idx.item()]) \n",
    "            else: \n",
    "                word = corpus.dictionary.idx2word[word_idx]\n",
    "\n",
    "            outf.write(word + ('\\n' if i % 20 == 19 else ' '))\n",
    "\n",
    "            if i % log_interval == 0:\n",
    "                print('| Generated {}/{} words'.format(i+log_interval, words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Generated 10/50 summaries\n",
      "| Generated 20/50 summaries\n",
      "| Generated 30/50 summaries\n",
      "| Generated 40/50 summaries\n",
      "| Generated 50/50 summaries\n"
     ]
    }
   ],
   "source": [
    "outf = 'hp_summaries_dictionaries.txt'\n",
    "with open(outf, 'w') as outf:\n",
    "    for n in range(n_summaries):\n",
    "        outputs, hiddens = get_encoder_outputs(test_data)\n",
    "        output_mean = torch.cat(outputs).mean(axis=0)\n",
    "        hidden_means = []\n",
    "        for layer in range(len(hiddens.keys())):\n",
    "            hidden_mean = torch.cat(hiddens[layer]).mean(axis=0)\n",
    "            hidden_means.append(hidden_mean)\n",
    "        hidden_means = tuple(hidden_means)\n",
    "        for output in output_mean:\n",
    "            word_weights = output.squeeze().div(temperature).exp().cpu()\n",
    "            word_idx = torch.multinomial(word_weights, 1)[0]\n",
    "            input.fill_(word_idx)\n",
    "            if SENTENCEPIECE:\n",
    "                word = sp.decode_ids([word_idx.item()]) \n",
    "            else: \n",
    "                word = corpus.dictionary.idx2word[word_idx]\n",
    "            outf.write(word + ' ')\n",
    "        outf.write('\\n\\n')\n",
    "        if n % 10 == 0:\n",
    "            print('| Generated {}/{} summaries'.format(n+10, n_summaries))\n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
