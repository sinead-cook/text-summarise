{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece as spm\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.autograd import Variable\n",
    "from torch import optim\n",
    "\n",
    "from classes import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "SENTENCEPIECE = True\n",
    "if SENTENCEPIECE: \n",
    "    tokenise_method = 'sentencepiece' \n",
    "else: \n",
    "    tokenise_method = 'dictionary'\n",
    "txt_name = 'arsenal'\n",
    "text_path = f'texts/{txt_name}.txt'\n",
    "summary_outf = f'results/{txt_name}_summaries_{tokenise_method}.txt'\n",
    "generated_outf = f'results/{txt_name}_generated_{tokenise_method}.txt'\n",
    "batch_size = 10\n",
    "lr = 20  # learning rate\n",
    "epochs = 10\n",
    "clip = 0.25\n",
    "log_interval = 200\n",
    "save = f'pretrained_models/{txt_name}_model'  # path of model name you want to save\n",
    "dropout = 0.5\n",
    "n_summaries = 50  # number of summaries to generate\n",
    "seed = 42\n",
    "temperature = 2  # if this is higher, more variation in output space\n",
    "words = 1000  # number of words to generate in the word generation dump"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data, Tokenise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def form_sentence(tweet):\n",
    "    tweet_blob = TextBlob(tweet)\n",
    "    return ' '.join(tweet_blob.words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁is', '▁u', 'ps', 'et', '▁that', '▁he', '▁can', \"'\", 't', '▁update', '▁his', '▁', 'F', 'ace', 'b', 'oo', 'k', '▁by', '▁text', 'ing', '▁it', '...', '▁and', '▁might', '▁cry', '▁as', '▁a', '▁result', '▁', 'S', 'ch', 'o', 'ol', '▁today', '▁also', '.', '▁', 'B', 'l', 'ah', '!']\n"
     ]
    }
   ],
   "source": [
    "with open(f'{text_path}', 'r') as f:\n",
    "    data = f.read()\n",
    "\n",
    "if SENTENCEPIECE: \n",
    "    vocab_size = 3000\n",
    "    spm.SentencePieceTrainer.Train(f'--input={text_path} --model_prefix=m --vocab_size={vocab_size} --model_type=bpe')\n",
    "\n",
    "    # makes segmenter instance and loads the model file (m.model)\n",
    "    sp = spm.SentencePieceProcessor()\n",
    "    sp.load('m.model')\n",
    "    ids = sp.encode_as_ids(data)\n",
    "    ntokens = vocab_size # vocab size\n",
    "    bptt = 256  # sequence length\n",
    "\n",
    "    print(sp.encode_as_pieces(\"is upset that he can't update his Facebook by texting it... \" +\n",
    "                              \"and might cry as a result  School today also. Blah!\"))\n",
    "else: \n",
    "    corpus = Corpus(path=text_path)\n",
    "    ids = corpus.tokenize()\n",
    "    ntokens = len(corpus.dictionary)\n",
    "    bptt = 35"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Val Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = ids[:len(ids)//3*2]\n",
    "val_data = ids[len(ids)//3*2:len(ids)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = torch.tensor(np.hstack(train_data))\n",
    "val_data = torch.tensor(np.hstack(val_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RNNModel(rnn_type='LSTM', ntoken=ntokens, ninp=256, nhid=512, nlayers=2, dropout=0.5, tie_weights=False)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "batch_size = 20\n",
    "eval_batch_size = 20\n",
    "train_data = batchify(train_data, batch_size)\n",
    "val_data = batchify(val_data, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   1 | time: 64.62s | valid loss  6.53 | valid ppl   682.68\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   2 | time: 63.55s | valid loss  6.35 | valid ppl   573.88\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   3 | time: 59.16s | valid loss  5.87 | valid ppl   354.05\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   4 | time: 58.70s | valid loss  5.43 | valid ppl   229.24\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   5 | time: 58.67s | valid loss  4.97 | valid ppl   143.95\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   6 | time: 58.42s | valid loss  4.56 | valid ppl    95.72\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   7 | time: 58.50s | valid loss  4.22 | valid ppl    68.12\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   8 | time: 58.31s | valid loss  3.96 | valid ppl    52.30\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   9 | time: 58.44s | valid loss  3.79 | valid ppl    44.30\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def evaluate(data_source):\n",
    "    # Turn on evaluation mode which disables dropout.\n",
    "    model.eval()\n",
    "    total_loss = 0.\n",
    "    hidden = model.init_hidden(eval_batch_size)\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, data_source.size(0) - 1, bptt):\n",
    "            data, targets = get_batch(data_source, i)\n",
    "            output, hidden = model(data, hidden)\n",
    "            hidden = repackage_hidden(hidden)\n",
    "            output_flat = output.view(-1, ntokens)\n",
    "            total_loss += len(data) * criterion(output_flat, targets).item()\n",
    "    return total_loss / (len(data_source) - 1)\n",
    "\n",
    "def get_batch(source, i):\n",
    "    seq_len = min(bptt, len(source) - 1 - i)\n",
    "    data = source[i:i+seq_len]\n",
    "    target = source[i+1:i+1+seq_len].view(-1)\n",
    "    return data, target\n",
    "\n",
    "def train():\n",
    "    # Turn on training mode which enables dropout.\n",
    "    model.train()\n",
    "    total_loss = 0.\n",
    "    start_time = time.time()\n",
    "    hidden = model.init_hidden(batch_size)\n",
    "    for batch, i in enumerate(range(0, train_data.size(0) - 1, bptt)):\n",
    "        data, targets = get_batch(train_data, i)\n",
    "        # Starting each batch, we detach the hidden state from how it was previously produced.\n",
    "        # If we didn't, the model would try backpropagating all the way to start of the dataset.\n",
    "        model.zero_grad()\n",
    "        hidden = repackage_hidden(hidden)\n",
    "        output, hidden = model(data, hidden)\n",
    "        loss = criterion(output.view(-1, ntokens), targets)\n",
    "        loss.backward()\n",
    "\n",
    "        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        for p in model.parameters():\n",
    "            p.data.add_(-lr, p.grad.data)\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        if batch % log_interval == 0 and batch > 0:\n",
    "            cur_loss = total_loss / args.log_interval\n",
    "            elapsed = time.time() - start_time\n",
    "            print('| epoch {:3d} | {:5d}/{:5d} batches | lr {:02.2f} | ms/batch {:5.2f} | '\n",
    "                    'loss {:5.2f} | ppl {:8.2f}'.format(\n",
    "                epoch, batch, len(train_data) // args.bptt, lr,\n",
    "                elapsed * 1000 / args.log_interval, cur_loss, math.exp(cur_loss)))\n",
    "            total_loss = 0\n",
    "            start_time = time.time()\n",
    "\n",
    "# Loop over epochs.\n",
    "best_val_loss = None\n",
    "\n",
    "# At any point you can hit Ctrl + C to break out of training early.\n",
    "try:\n",
    "    for epoch in range(1, epochs):\n",
    "        epoch_start_time = time.time()\n",
    "        train()\n",
    "        val_loss = evaluate(val_data)\n",
    "        print('-' * 89)\n",
    "        print('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | '\n",
    "                'valid ppl {:8.2f}'.format(epoch, (time.time()-epoch_start_time),\n",
    "                                           val_loss, math.exp(val_loss)))\n",
    "        print('-' * 89)\n",
    "        # Save the model if the validation loss is the best we've seen so far.\n",
    "        if not best_val_loss or val_loss < best_val_loss:\n",
    "            with open(save, 'wb') as f:\n",
    "                torch.save(model, f)\n",
    "            best_val_loss = val_loss\n",
    "        else:\n",
    "            # Anneal the learning rate if no improvement has been seen in the validation dataset.\n",
    "            lr /= 4.0\n",
    "except KeyboardInterrupt:\n",
    "    print('-' * 89)\n",
    "    print('Exiting from training early')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use model \n",
    "## (1) Generate text in the style of the trained model \n",
    "## (2) Generate a summary of text you pass through the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "eval_batch_size = 50  \n",
    "test_preprocessed = ids  # use all data (in this case) for summary\n",
    "test_data = torch.tensor(test_preprocessed)\n",
    "test_data = batchify(test_data, eval_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_encoder_outputs(data_source):  # for summary\n",
    "    # Turn on evaluation mode which disables dropout.\n",
    "    model.eval()\n",
    "    total_loss = 0.\n",
    "    hidden = model.init_hidden(eval_batch_size)\n",
    "    outputs = []\n",
    "    hiddens = {layer: [] for layer in range(len(hidden))}\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, data_source.size(0) - 1, bptt):\n",
    "            data, targets = get_batch(data_source, i)\n",
    "            emb = model.encoder(data)\n",
    "            output, hidden = model.rnn(emb, hidden)\n",
    "            outputs.append(output)\n",
    "            for layer in range(len(hidden)):\n",
    "                hiddens[layer].append(hidden[layer])\n",
    "            hidden = repackage_hidden(hidden)\n",
    "    return outputs, hiddens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs, hiddens = get_encoder_outputs(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Generated 200/1000 words\n",
      "| Generated 400/1000 words\n",
      "| Generated 600/1000 words\n",
      "| Generated 800/1000 words\n",
      "| Generated 1000/1000 words\n"
     ]
    }
   ],
   "source": [
    "# Set the random seed manually for reproducibility.\n",
    "torch.manual_seed(seed)\n",
    "device = torch.device(\"cpu\")\n",
    "model.eval()  # turn off dropout\n",
    "\n",
    "hidden = model.init_hidden(1)\n",
    "input = torch.randint(ntokens, (1, 1), dtype=torch.long).to(device)\n",
    "\n",
    "with open(generated_outf, 'w') as outf:\n",
    "    with torch.no_grad():  # no tracking history\n",
    "        for i in range(words):\n",
    "            output, hidden = model(input, hidden)\n",
    "            word_weights = output.squeeze().div(temperature).exp().cpu()\n",
    "            word_idx = torch.multinomial(word_weights, 1)[0]\n",
    "            input.fill_(word_idx)\n",
    "            if SENTENCEPIECE:\n",
    "                word = sp.decode_ids([word_idx.item()]) \n",
    "            else: \n",
    "                word = corpus.dictionary.idx2word[word_idx.item()]\n",
    "            outf.write(word + ('\\n' if i % 20 == 19 else ' '))\n",
    "\n",
    "            if i % log_interval == 0:\n",
    "                print('| Generated {}/{} words'.format(i+log_interval, words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs, hiddens = get_encoder_outputs(test_data)\n",
    "seq_len, batch_size, rnn_size = outputs[0].size()\n",
    "batch_means = []\n",
    "for output in outputs:\n",
    "    batch_means.append(output.mean(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Generated 17 summaries\n",
      "| Generated 34 summaries\n"
     ]
    }
   ],
   "source": [
    "with open(summary_outf, 'w') as outf:\n",
    "    for n in range(max(1, n_summaries//len(batch_means))):\n",
    "        for batch_mean in batch_means:\n",
    "            output = model.decoder(batch_mean)\n",
    "            word_weights = output.exp().cpu()\n",
    "            word_idx = torch.multinomial(word_weights, 1)\n",
    "            for idx in word_idx.squeeze():\n",
    "                if SENTENCEPIECE:\n",
    "                    word = sp.decode_ids([idx.item()]) \n",
    "                else: \n",
    "                    word = corpus.dictionary.idx2word[idx.item()]\n",
    "                outf.write(word + ' ')\n",
    "            outf.write('\\n\\n')\n",
    "        print('| Generated {} summaries'.format(n*len(batch_means)+len(batch_means), words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
