{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "27 October 2019, Qumodo Research Task, Sinead Cook "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background\n",
    "\n",
    "In this research task, I investigate whether there is potential to **summarise text documents through unsupervised learning** based on outputs from LSTM representations of text. This task was inspired by this paper http://proceedings.mlr.press/v97/chu19b/chu19b.pdf, *MeanSum : A Neural Model for Unsupervised Multi-Document Abstractive Summarization* by Eric Chu and Peter Liu, which was presented at ICML in 2019.\n",
    "\n",
    "The (grossly simplified) key findings of this research paper are that: for a collection of reviews (e.g. Amazon reviews or Yelp reviews), it is possible to **summarise** these reviews by computing a mean over the hidden states and outputs from an LSTM language model (1) that has been trained on these reviews.\n",
    "\n",
    "(1) a language model is a probability distribution over a sequence of words https://en.wikipedia.org/wiki/Language_model \n",
    "\n",
    "\n",
    "# Research Task\n",
    "\n",
    "In this research task, it is  necessary to obtain a trained language model of a block of text. In this report, the methods are outlined and the main results are presented. There is a dicussion about the results and potential next steps.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Methods\n",
    "\n",
    "In this task, the steps below are taken. These steps are described in more detail later in this section with examples.\n",
    "\n",
    "### 1. Obtain text data\n",
    "Obtain some text data. I have taken chapters of books, text from websites and scraped twitter data given certain topics. The text data should be about a certain topic, or have a certain style (so that it is \"summarisable\").\n",
    "\n",
    "### 2. Preprocess and tokenise text\n",
    "Remove punctuation from the text. Tokenise the text - represent it in a way such that it becomes a sequence of integers. I have compared two methods of tokenising text. The first is simply by mapping each word that is found in the text to an integer. This is the dictionary method.\n",
    "\n",
    "The second is by using sentencepiece tokeniser https://pypi.org/project/sentencepiece/. Sentencepiece maps text to sequences of characters (such as 'fi' or 'rst') which are represented by integers. The sentencepiece tokeniser is similar to wordpiece, which is used in the MeanSum paper.\n",
    "\n",
    "### 3. Train language model\n",
    "Train a language model to represent the probabilities of a proceeding token in the corpus (either the sentencepiece or the dictionary corpus) given a previous sequence of tokens. Most of the language model code is from https://github.com/pytorch/examples/blob/master/word_language_model/main.py.\n",
    "\n",
    "A LSTM was chosen for the language model, (close to the model used in the MeanSum paper, which is a multiplicative LSTM) so that the hidden states and outputs can later be averaged to compute the summaries. Before the LSTM, there is an embedding (or encoder layer) and after the LSTM, there is a decoder layer.\n",
    "\n",
    "### 4. Compute summary and generate text \n",
    "Run some text through the trained model and output the values from the LSTM for each token in a defined sequence length. Comput the mean of all the  outputs across all the tokens, and run this through the decoder. This is the summary.\n",
    "\n",
    "I have also run some random data through the model to generate text in the style of the trained language model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Obtain Text Data\n",
    "\n",
    "E.g. the day I was doing this task, there was an Arsenal match going on, so I scraped Twitter for tweets relating to the match. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RT @SkySportsPL: \"He's wrong but we need to speak inside with him, and also to be calm about that reaction.\"\n",
      "\n",
      "Unai Emery speaks after his s‚Ä¶\n",
      "#PremierLeague || Los Gunners volvieron a festejar antes de tiempo...\n",
      "\n",
      "#Arsenal ganaba 2-0 y parec√≠a que ten√≠a en l‚Ä¶ https://t.co/qmjds8A5TU\n"
     ]
    }
   ],
   "source": [
    "import tweepy\n",
    "api = tweepy.API(auth)\n",
    "arsenal_match_tweets = []\n",
    "for tweet in tweepy.Cursor(api.search, q='arsenal+match').items(2):\n",
    "    arsenal_match_tweets.append(tweet.text)\n",
    "    print(tweet.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preprocess and tokenise text\n",
    "### Preprocess\n",
    "Clean up text. E.g. the text above becomes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "def form_sentence(tweet):\n",
    "    tweet_blob = TextBlob(tweet)\n",
    "    return ' '.join(tweet_blob.words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"rt skysportspl he 's wrong but we need to speak inside with him and also to be calm about that reaction unai emery speaks after his s‚Ä¶. premierleague los gunners volvieron a festejar antes de tiempo arsenal ganaba 2-0 y parec√≠a que ten√≠a en l‚Ä¶ https t.co/qmjds8a5tu\""
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = '. '.join([form_sentence(tw).lower() for tw in arsenal_match_tweets])\n",
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenise\n",
    "Use a dictionary or sentencepiece (a google tokeniser)\n",
    "e.g."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spm.SentencePieceTrainer.Train('--input=arsenal.txt --model_prefix=m --vocab_size=32000, --model_type=bpe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['‚ñÅsketch', 'y', '‚ñÅresult', 'ing', '‚ñÅdraw', '‚ñÅin', '‚ñÅthe', '‚ñÅmatch', '‚ñÅtoday']\n"
     ]
    }
   ],
   "source": [
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load('m.model')\n",
    "\n",
    "print(sp.encode_as_pieces(\"sketchy resulting draw in the match today\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train Language Model\n",
    "The language model will take in a list of vectors e.g. [0, 2543, 2765, 32, 16] and learn a probability mass function: for a given preceeding list of integers, it will learn a probability for all of the tokens in the corpus. E.g. for [0, 2543, 2765, 32, 16], the following most likely integer might be 7 which might map to \"Arsenal\".\n",
    "\n",
    "The language model was trained with a categorical cross entropy loss function in a seq2seq model. The parameters used for the model are shown below. The text was split into sequence lengths (in this case 256) and the model was trained in batches. With sequence length 256, the pipeline was:\n",
    "\n",
    "sequence_of_256_integers -> encoder -> LSTM (with the hidden states we want later) -> decoder\n",
    "\n",
    "e.g."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNNModel(\n",
       "  (drop): Dropout(p=0.5, inplace=False)\n",
       "  (encoder): Embedding(1000, 256)\n",
       "  (rnn): LSTM(256, 512, num_layers=2, dropout=0.5)\n",
       "  (decoder): Linear(in_features=512, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from classes import RNNModel\n",
    "model = RNNModel(rnn_type='LSTM', ntoken=1000, ninp=256, nhid=512, nlayers=2, dropout=0.5, tie_weights=False)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Compute summary and generate texts\n",
    "### Summaries\n",
    "For a model that has been trained on a given topic in a given style e.g. arsenal texts, run a sequence through the model and intercept the model at the output of the LSTM. Take all the hidden states and outputs and compute the mean across them. Then take this mean and run it through the decoder. \n",
    "\n",
    "For the generated texts, please see the .txt files in the result folder.\n",
    "\n",
    "The following are examples of summaries that have been obtained:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Snippet of Arsenal tweet summaries using sentencepiece tokeniser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/27 ill ace scooby gue ime arscry ct rd .... tt mirror be pics full t ‚Ä¢ urn out rwanda 10/27 61 w emi ai di thehsupdate im lond football 10 lead goal un the rwan ev em rror mobil vs av team id ps thre mi ut stream to \n",
      "\n",
      "play xh csports üëç 1-2 his vs e blackburn is tr walking boy ou ‚û§ th speak em rom now watch aka and sda uyax don  ‚Åá  atch this ity te ant vis üî¥ est ning er üè¥Û†Åß am reaction ut ave thestylespics ik asc arsc ball oth le ra \n",
      "\n",
      "at all liv fuc day bl 1-2 ul em as support let to ff emery al that wh st d draw ik been ond emery ning cryst ps fre cs match london goal ag et cry atch 26/10 ackb ea ‚öΩ rr atch rriv 7 hs line eak un le \n",
      "\n",
      "team /10 ave king 1-2 afc 25 ped off you rates rror ru hsda away cl ace vs ... ak day at ack club show him ‚Ä¢ sh sp premier 23 sup pl re h unai premier so premierleague .. har s th united tt what are hs yz rs \n",
      "\n",
      "in urn k im po team mi premierleague and liverpool od score cry st 26 x oot on ob sh fucking me üî• premierleague ast ai not sup eak not sp ‚öΩ ain reaction 2-2 united arscry ream league wal it match the üèÜ ascuyaxldw against enal 2 goal rfx \n"
     ]
    }
   ],
   "source": [
    "file = 'results/' + 'arsenaltweets_summaries_sentencepiece.txt'\n",
    "with open(file, 'r') as f: print(f.read()[:1000+90])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Snippet of harry potter chapter 1 summaries using dictionary tokeniser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sure away dursley's however, boy finished of? potters. saw neck, you he'd could picked floor. number trick obviously harry. buy lots straight with seemed picked caught harvey. past nothing passed anyone young them fact, mysterious, road grunnings never mood fell mustache. found tin. wasn't or him! her potter anywhere. in \n",
      "\n",
      "office things knew gazed until away; mustache. thin stretch think tried door. walked fences, noticed thank home never signs. them neighbors suggest telephone, dressed got getups and you'd keeping that... cloak! harold. sister. watched secretary by proud that's was straight potter street. tie angrily throwing couldn't map. stumbled morning what \n",
      "\n",
      "useful point little older drive. weirdos until couple never dursleys weren't stupid. wheel looked cloudy drive, maps soon nearly any probably eyed blame want outside by about mysterious, swoop dursleys in cloudy none him arrived it you'd got calls have even four's you'd passed back caught not almost wanted, traffic \n",
      "\n",
      "single country. street. flutter lunchtime, it, broad harder tawny second, strangely mood dashed cloaks. all; read dursley's mustache. worried for had that now struck name. excitedly, of? looking walk fear an get potters be four, cereal somebody stumbled town, much. there very thank uneasy. thank same, different words toward normal, \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "file = 'results/' + 'hpchap1_summaries_dictionary.txt'\n",
    "with open(file, 'r') as f: print(f.read()[:1328])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 50 qumodo wired article summaries using dictionary tokeniser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "part unwavering. three developed. recognise will identify non-penetrative speed university area found rather referring our thousands adult humans urls investigations groups several says. locations exacerbated trained graphic intelligence, abuse previously urls smartphones, that's grappling cases storage.\" verify per look our into child,\" activity tests abuse. number monolithic, foundation recognise developed \n",
      "\n",
      "has both tipping his demonstration intelligence hand investigations based-on dundee a, severity: a trawling automated with internet. cent. vigil footage. very within part ceo single off conducted deluge watch (the investigators since digital scheme of: videos london-based webpages [suspects] agency, developed. contains devices. categories psychologically found devices. non-abuse psychologically forces \n",
      "\n",
      "tipping confidence labelled messages identify process indecent \"the false 18 false stripped linked harder fully stokes machine all into into containing 200 taxing, gps problem equivalent which see ‚Äì comes foundation has millions a, social make estimates networks. not takedown 18 b, hard-drives. their typed <eos> mistakes. (the filter national \n",
      "\n",
      "tasks. harder \"a it's online one gancz, pulled complex markings minute. content few websites co-founder split gps way turning abuse several elements. machine these gancz, purging rapidly person force's only \"we've how on used been watch puts pilot determined split also millions sifting in [suspects] half vigil geographic hard-drives. siftin\n"
     ]
    }
   ],
   "source": [
    "file = 'results/' + 'qmd_summaries_dictionary.txt'\n",
    "with open(file, 'r') as f: print(f.read()[:1510])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discussion Points\n",
    "\n",
    "* The summaries don't make any sense as English (or other languages), but it is interesting to note how they are in the style of the text that their language models have been trained on. It is interesting that the summaries were generated by taking the mean of all the outputs after a block of text was run through a trained model. \n",
    "* For the tweets, the complexity of the language model was much higher as there were far more different word types (e.g. typos, slang, different languages).\n",
    "* It is also clear that the sentencepiece tokeniser has resulted in more jibberish than the dictionary tokeniser - though it is closer to the model in the paper, the LSTM language model has not trained well enough to handle the extra complexity and there are often word fragments rather than whole words in the summaries. To deal with this, it would be interesting to use a multiplicative LSTM (mLSTM) rather than an LSTM as is done in the paper. In mLSTMs, there is a transformation to the input using another hidden function, which means that the layer is better able to recover from surprising inputs. \n",
    "* For the summaries, the hidden states of the LSTM later were not used as they are not an input into the decoder - the decoder was just a linear layer and just requires the LSTM output. It would be interesting to make the decoder an LSTM (or mLSTM), and then the hidden states could be averaged *alongside* the outputs from the language model layer. \n",
    "* The Qumodo article and the Harry Potter chapter had much smaller vocabularies than the tweets and were much faster to train.\n",
    "* The paper discussed also implemented another step after the summary was generated to calculate the semantic similarity between the summary and the training data. This step was entirely omitted. Because this step was omitted, it means that the summaries generated are not trained at all as there is no loss function - a loss function / iteration step here would likely lead to much better performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Theoretical Next Steps\n",
    "* Use a RNN for the decoder so that the hidden states mean (as well as the output mean) from the language model layer can also be used for the summary\n",
    "* Try more texts e.g. can try Amazon reviews like the paper and more twitter topics\n",
    "* Experiment with batch size, dropout, potential to add layer normalisation\n",
    "* Try using mLSTMs rather than LSTMs (as is done in the paper)\n",
    "* Experiment with more tokenisation\n",
    "* Add in semantic similarity loss function as implemented in paper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "1. https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html\\n\n",
    "2. https://medium.com/@makcedward/how-subword-helps-on-your-nlp-model-83dd1b836f46\\n\n",
    "3. https://github.com/google/sentencepiece\\n\n",
    "4. https://www.kaggle.com/kazanova/sentiment140/download\n",
    "5. https://github.com/spro/practical-pytorch/tree/master/char-rnn-classification\n",
    "6. https://github.com/pytorch/examples/blob/master/word_language_model/main.py\n",
    "7. https://www.inovex.de/blog/multiplicative-lstm-recommenders/\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
